{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38386057-612e-4b5d-a8e7-194043ff8750",
   "metadata": {},
   "outputs": [],
   "source": [
    "studentsScriptFileName = \"Actual Test Scripts (after override).xlsx\"\n",
    "standAnswerFileName = \"Actual Test Solution_override.xlsx\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f0deb9-8062-487a-8ccb-e48469b0fde2",
   "metadata": {},
   "source": [
    "# Prerequisite\n",
    "Setup new Connda Environment\n",
    "1. New terminal.\n",
    "2. Run ```cd sagemaker-studiolab-notebooks/ ```\n",
    "3. Run ```conda env create -f env_basic.yaml ```\n",
    "4. Refresh your jupter notebook webpage.\n",
    "5. Select the Kenel \"basic\".\n",
    "\n",
    "To add new packages\n",
    "1. Update \"env_basic.yaml\".\n",
    "2. Run ```conda env update -f env_basic.yaml ```\n",
    "\n",
    "Add AWS Academy Learner Lab and remember to renew it every 4 hours!\n",
    "1. Rename AWSAcademyLeanerLab-template.config to AWSAcademyLeanerLab.config.\n",
    "2. Copy AWS CLI credentials from AWS Academy Learner Lab.\n",
    "3. Paste it in AWSAcademyLeanerLab.config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1621a27-5f61-4f88-b98b-a21a75545f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "\n",
    "canCallAWS = False\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read(\"AWSAcademyLeanerLab.config\")\n",
    "config.sections()\n",
    "awsAccessKeyId = config[\"default\"][\"aws_access_key_id\"]\n",
    "awsSecretAccessKeyId = config[\"default\"][\"aws_secret_access_key\"]\n",
    "awsSessionToken = config[\"default\"][\"aws_session_token\"]\n",
    "if awsAccessKeyId == \"\" or awsSecretAccessKeyId == \"\" or awsSessionToken == \"\":\n",
    "    print(\"Missing AWSAcademyLeanerLab credentials\")\n",
    "else:\n",
    "    import boto3\n",
    "\n",
    "    boto3.setup_default_session(\n",
    "        aws_access_key_id=awsAccessKeyId,\n",
    "        aws_secret_access_key=awsSecretAccessKeyId,\n",
    "        aws_session_token=awsSessionToken,\n",
    "        region_name=\"us-east-1\",\n",
    "    )\n",
    "    awsAccount = boto3.client(\"sts\").get_caller_identity().get(\"Account\")\n",
    "    print(\"Your AWS Account Number: \" + awsAccount)\n",
    "    canCallAWS = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7de114-5d49-47f6-a77d-a5c628a5496d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.cluster.hierarchy as spc\n",
    "from scipy import stats\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e037c150-7fac-459d-8ae5-2aa1e53db321",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_script = pd.read_excel(\n",
    "    os.path.join(os.getcwd(), \"data\", studentsScriptFileName), sheet_name=None\n",
    ")\n",
    "df_answer = pd.read_excel(\n",
    "    os.path.join(os.getcwd(), \"data\", standAnswerFileName), sheet_name=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b139e037-63ab-41f0-9063-ba7a6644fd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_answer[\"DocumentValue\"].head()\n",
    "df_answer[\"DocumentValue\"].columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850f943c-e9c8-41b3-8886-60b14571aa5c",
   "metadata": {},
   "source": [
    "Columns appear in Standard answer but not in Student script, whihc is missing from Amazon Textract and probably because all students do not answer those question!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f15578-d66b-49e7-a9a4-811374563c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_answer[\"DocumentValue\"].columns.difference(df_script[\"PageAnswerGeometry\"].columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4133ffe8-daef-4a5b-9338-c21f6e0a1cb7",
   "metadata": {},
   "source": [
    "Columns appear in student script but not in Standard answer, and you need to updated the answer excel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56db65f-29bc-4157-97a1-7e61ce053cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_script[\"PageAnswerGeometry\"].columns.difference(df_answer[\"DocumentValue\"].columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae78fb6-9b70-47ba-b619-1c155ea1234c",
   "metadata": {},
   "source": [
    "Common columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c21a0f-eb3a-45b2-930b-ef7be108eba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = (\n",
    "    df_script[\"PageAnswerGeometry\"]\n",
    "    .columns.intersection(df_answer[\"DocumentValue\"].columns)\n",
    "    .to_list()\n",
    ")\n",
    "columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98b81f0-4610-49be-ad26-885d21849ce5",
   "metadata": {},
   "source": [
    "Apppend Student Name and ID columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed47a556-32f3-435b-952a-4087aff34bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns.insert(0, \"Name\")\n",
    "columns.insert(0, \"ID\")\n",
    "columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eabc012-d11b-42e0-91be-32c1c080f3d4",
   "metadata": {},
   "source": [
    "Remove columns do not show in answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d28fbae-f99b-4a8a-9de0-56c6cf660285",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in df_answer.keys():\n",
    "    df_script[key] = df_script[key][columns]\n",
    "df_answer.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0837b22d-a7ab-4e6f-98da-90919e872f07",
   "metadata": {},
   "source": [
    "# Preprocessing boundbox with margin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a2bfb5-4a67-4a7e-a467-b7f4ae9813d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from urllib.parse import urlparse\n",
    "import urllib.request\n",
    "from pathlib import Path\n",
    "import urllib.request\n",
    "\n",
    "\n",
    "path = Path(os.path.join(os.getcwd(), \"output\", \"grading_form\", \"images\"))\n",
    "path.mkdir(parents=True, exist_ok=True)\n",
    "images = {}\n",
    "for rowIndex, row in df_script[\"DocumentAnswerImage\"].iterrows():  # iterate over rows\n",
    "    for columnIndex, value in row.items():\n",
    "        if not pd.isna(value) and value not in images:\n",
    "            url = urlparse(value)\n",
    "            fileName = os.path.basename(url.path)\n",
    "            fileName = os.path.join(\"output\", \"grading_form\", \"images\", fileName)\n",
    "            if not os.path.isfile(fileName):\n",
    "                urllib.request.urlretrieve(value, fileName)\n",
    "            images[value] = fileName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2744d5-e9e7-4769-affd-c3b598976753",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "dir_path = Path(os.path.join(os.getcwd(), \"output\", \"grading_form\", \"images\"))\n",
    "\n",
    "\n",
    "def mapper(x):\n",
    "    if pd.isna(x) or x is None:\n",
    "        return None\n",
    "    j = json.loads(x)\n",
    "    if j is None:\n",
    "        return None\n",
    "    if \"BoundingBox\" in j:\n",
    "        if \"page\" in j:\n",
    "            boundingBox = j[\"BoundingBox\"]\n",
    "            path = Path(os.path.join(dir_path, \"p-\" + str(j[\"page\"]) + \".png\"))\n",
    "            with Image.open(path) as im:\n",
    "                # The crop method from the Image module takes four coordinates as input.\n",
    "                # The right can also be represented as (left+width)\n",
    "                # and lower can be represented as (upper+height).\n",
    "                pageWidth, pageHeight = im.size\n",
    "                boundingBox[\"page\"] = j[\"page\"] + 1\n",
    "                width, height = im.size\n",
    "\n",
    "                margin = boundingBox[\"Height\"] * 0.05\n",
    "                leftMargin = margin + 50\n",
    "                (left, top, width, height) = (\n",
    "                    max(pageWidth * boundingBox[\"Left\"] - leftMargin, 0),\n",
    "                    max(pageHeight * boundingBox[\"Top\"] - margin, 0),\n",
    "                    min(\n",
    "                        pageWidth * boundingBox[\"Width\"] + leftMargin + margin,\n",
    "                        pageWidth,\n",
    "                    ),\n",
    "                    min(pageHeight * boundingBox[\"Height\"] + 2 * margin, pageHeight),\n",
    "                )\n",
    "                return {\n",
    "                    \"left\": left,\n",
    "                    \"top\": top,\n",
    "                    \"height\": height,\n",
    "                    \"width\": width,\n",
    "                    \"page\": j[\"page\"],\n",
    "                }\n",
    "        try:\n",
    "            return json.loads(j)[\"BoundingBox\"]\n",
    "        except:\n",
    "            print(type(j))\n",
    "\n",
    "\n",
    "df_script[\"DocumentBoundingBox\"] = df_script[\"DocumentAnswerGeometry\"].applymap(mapper)\n",
    "df_answer[\"DocumentBoundingBox\"] = df_answer[\"DocumentAnswerGeometry\"].applymap(mapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddee029-c2f2-4141-9b67-73a08657a78e",
   "metadata": {},
   "source": [
    "# Preparation for the imputation the missing page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4f5063-6b73-491e-aebf-cb78be07c208",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper(x):\n",
    "    if pd.isna(x) or x is None:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "\n",
    "df_script[\"PageAnswer\"] = df_script[\"PageAnswerGeometry\"].applymap(mapper)\n",
    "df_script[\"PageAnswer\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b9bd51-3fde-4135-b3bb-46c42f6dc556",
   "metadata": {},
   "source": [
    "Create correlation matrix and apply clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cfbdb7-1c8b-4c1e-8c5b-26a8b22cab26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import scipy.cluster.hierarchy as sch\n",
    "\n",
    "\n",
    "def cluster_corr(corr_array, inplace=False):\n",
    "    \"\"\"\n",
    "    Rearranges the correlation matrix, corr_array, so that groups of highly\n",
    "    correlated variables are next to eachother\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    corr_array : pandas.DataFrame or numpy.ndarray\n",
    "        a NxN correlation matrix\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame or numpy.ndarray\n",
    "        a NxN correlation matrix with the columns and rows rearranged\n",
    "    \"\"\"\n",
    "    pairwise_distances = sch.distance.pdist(corr_array)\n",
    "    linkage = sch.linkage(pairwise_distances, method=\"complete\")\n",
    "    cluster_distance_threshold = pairwise_distances.max() / 2\n",
    "    idx_to_cluster_array = sch.fcluster(\n",
    "        linkage, cluster_distance_threshold, criterion=\"distance\"\n",
    "    )\n",
    "    idx = np.argsort(idx_to_cluster_array)\n",
    "\n",
    "    if not inplace:\n",
    "        corr_array = corr_array.copy()\n",
    "\n",
    "    if isinstance(corr_array, pd.DataFrame):\n",
    "        return corr_array.iloc[idx, :].T.iloc[idx, :]\n",
    "    return corr_array[idx, :][:, idx]\n",
    "\n",
    "\n",
    "corr = df_script[\"PageAnswer\"].corr(method=\"pearson\")\n",
    "corr = cluster_corr(corr)\n",
    "fig, ax = plt.subplots(figsize=(20, 20))\n",
    "ax = sns.heatmap(\n",
    "    corr,\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    "    center=0,\n",
    "    cmap=sns.diverging_palette(20, 220, n=200),\n",
    "    square=True,\n",
    ")\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment=\"right\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d9d314-fc11-4852-a5c9-fc02ba34828c",
   "metadata": {},
   "source": [
    "A Page contains a set of question and it is a group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecab8fa9-924c-4b9c-b7f5-17781639cf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdist = spc.distance.pdist(corr)\n",
    "linkage = spc.linkage(pdist, method=\"complete\")\n",
    "idx = spc.fcluster(linkage, 0.5 * pdist.max(), \"distance\")\n",
    "\n",
    "grouping = pd.DataFrame(data={\"question\": corr.columns, \"group\": idx})\n",
    "grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778e91bd-5b4a-4481-9654-d5bdbe3acecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_group = (\n",
    "    grouping.groupby(\"group\")[\"question\"].apply(list).reset_index(name=\"questions\")\n",
    ")\n",
    "question_group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdb2f2d-c9a0-46c7-a8a3-e2aa3eba0cb5",
   "metadata": {},
   "source": [
    "Create ignore flag if the average of correlation is less than 0.5. Because Amazon Textract may capture some noise questions and it will become a group, the noise group correlation is very low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36edbdbe-7232-4ea6-aebe-33b61f6f4bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subgroup_average(questions, corr):\n",
    "    subdf = corr[questions]\n",
    "    return subdf[subdf.index.isin(questions)].mean().mean()\n",
    "\n",
    "\n",
    "question_group[\"mean\"] = question_group[\"questions\"].apply(\n",
    "    lambda x: get_subgroup_average(x, corr)\n",
    ")\n",
    "question_group[\"ignore\"] = question_group[\"mean\"] < 0.5\n",
    "question_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf39008-1199-4f8a-be9b-ee38e4337083",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_mapping = pd.merge(grouping, question_group, how=\"inner\", on=\"group\").rename(\n",
    "    columns={\"questions\": \"question_set\"}\n",
    ")\n",
    "question_mapping = question_mapping.set_index(\"question\")\n",
    "question_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736ea132-4ee0-4f36-a98b-9d39bf489cb3",
   "metadata": {},
   "source": [
    "By checking question_set, we can impute the missing page number by checking the page in the same set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c96b8af-d637-4e64-afce-b11795c6f9e6",
   "metadata": {},
   "source": [
    "# Preparation for the imputation the missing bounding box"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ab0423-7d5d-4074-92e8-ee601da2e9cb",
   "metadata": {},
   "source": [
    "Exclude the ignore columns and calculates the ratio of missing value for each question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ef5630-8c56-4e4f-8e8e-8aa0cb4c45c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = question_mapping[question_mapping[\"ignore\"] == False].index\n",
    "df_script[\"DocumentBoundingBoxFiltered\"] = df_script[\"DocumentBoundingBox\"][columns]\n",
    "df_script[\"DocumentBoundingBoxFiltered\"].isna().mean().round(4) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b3ba4c-7266-488b-8a6d-9be3645146b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trim_mean(box, field):\n",
    "    return stats.trim_mean(list(map(lambda x: x[field], box)), 0.05)\n",
    "\n",
    "\n",
    "def trimed_mean_bound_box(boxes):\n",
    "    box = list(filter(lambda box: box is not None, boxes))\n",
    "    height = get_trim_mean(box, \"height\")\n",
    "    left = get_trim_mean(box, \"left\")\n",
    "    top = get_trim_mean(box, \"top\")\n",
    "    width = get_trim_mean(box, \"width\")\n",
    "\n",
    "    return {\"height\": height, \"left\": left, \"top\": top, \"width\": width}\n",
    "\n",
    "\n",
    "trimedMeanBoundBoxes = df_script[\"DocumentBoundingBoxFiltered\"].agg(\n",
    "    trimed_mean_bound_box, axis=0\n",
    ")\n",
    "frame = {\"boundingBox\": trimedMeanBoundBoxes}\n",
    "estimatedBoundBoxes = pd.DataFrame(frame)\n",
    "estimatedBoundBoxes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188d3059-9860-4ddb-9aa1-b8089e3a0a04",
   "metadata": {},
   "source": [
    "# Imputation for the missing Bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14c56ec-f14a-413a-8d54-922ef09e3907",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_script[\"DocumentBoundingBoxImpute\"] = df_script[\"DocumentBoundingBox\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5850fbf-2d24-41be-80b7-5a27d3b34d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mode\n",
    "\n",
    "\n",
    "def impute_page_and_boundbox(df, row_number, row):\n",
    "    for key, cell in row.items():\n",
    "        if cell is None:\n",
    "            estimatedBoundBox = estimatedBoundBoxes.loc[[key]][\"boundingBox\"][0]\n",
    "            questionSet = question_mapping.loc[[key]][\"question_set\"][0]\n",
    "            page = mode(\n",
    "                map(\n",
    "                    lambda x: x[\"page\"],\n",
    "                    filter(lambda x: x is not None, map(lambda q: row[q], questionSet)),\n",
    "                )\n",
    "            )\n",
    "            estimatedBoundBox = {\n",
    "                \"left\": estimatedBoundBox[\"left\"],\n",
    "                \"top\": estimatedBoundBox[\"top\"],\n",
    "                \"height\": estimatedBoundBox[\"height\"],\n",
    "                \"width\": estimatedBoundBox[\"width\"],\n",
    "                \"page\": page,\n",
    "            }\n",
    "            df.at[row_number, key] = estimatedBoundBox\n",
    "\n",
    "\n",
    "for i, j in df_script[\"DocumentBoundingBox\"].iterrows():\n",
    "    impute_page_and_boundbox(df_script[\"DocumentBoundingBoxImpute\"], i, j)\n",
    "df_script[\"DocumentBoundingBoxImpute\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b9a9dd-3956-4eda-8561-7a5c15f50caa",
   "metadata": {},
   "source": [
    "# Imputation Image path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3449c9ca-6ff7-4dbc-9a67-c89bf29f249d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_script[\"DocumentAnswerImageLocal\"] = df_script[\"DocumentAnswerImage\"]\n",
    "df_script[\"DocumentAnswerImageLocal\"] = df_script[\"DocumentAnswerImageLocal\"].applymap(\n",
    "    lambda x: images[x], na_action=\"ignore\"\n",
    ")\n",
    "\n",
    "df_script[\"DocumentAnswerImageLocal\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34a8c11-88d6-41b5-852c-73ddaf457b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "path = Path(os.path.join(os.getcwd(), \"output\", \"grading_form\", \"images\"))\n",
    "\n",
    "\n",
    "def impute_image_path(df, row_number, row):\n",
    "    for key, cell in row.items():\n",
    "        if cell is None:\n",
    "            questionSet = question_mapping.loc[[key]][\"question_set\"][0]\n",
    "            page = mode(\n",
    "                map(\n",
    "                    lambda x: x[\"page\"],\n",
    "                    filter(lambda x: x is not None, map(lambda q: row[q], questionSet)),\n",
    "                )\n",
    "            )\n",
    "            df.at[row_number, key] = os.path.join(\n",
    "                \"output\", \"grading_form\", \"images\", \"p-\" + str(page) + \".png\"\n",
    "            )\n",
    "\n",
    "\n",
    "for i, j in df_script[\"DocumentBoundingBox\"].iterrows():\n",
    "    impute_image_path(df_script[\"DocumentAnswerImageLocal\"], i, j)\n",
    "df_script[\"DocumentAnswerImageLocal\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca8d7d2-6ccc-4638-ab56-00c807d425a8",
   "metadata": {},
   "source": [
    "# Preprocessing Choice style questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabdad82-5b7b-4911-8013-40b84ada61fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimatedBoundBoxesOptions = estimatedBoundBoxes[\n",
    "    estimatedBoundBoxes.index.str.endswith(\"-Yes\")\n",
    "    | estimatedBoundBoxes.index.str.endswith(\"-No\")\n",
    "    | estimatedBoundBoxes.index.str.endswith(\"-A\")\n",
    "    | estimatedBoundBoxes.index.str.endswith(\"-B\")\n",
    "    | estimatedBoundBoxes.index.str.endswith(\"-C\")\n",
    "    | estimatedBoundBoxes.index.str.endswith(\"-D\")\n",
    "    | estimatedBoundBoxes.index.str.endswith(\"-E\")\n",
    "].copy()\n",
    "estimatedBoundBoxesOptions[\"question\"] = estimatedBoundBoxesOptions.index\n",
    "estimatedBoundBoxesOptions[[\"question\", \"option\"]] = estimatedBoundBoxesOptions[\n",
    "    \"question\"\n",
    "].str.split(\"-\", 1, expand=True)\n",
    "estimatedBoundBoxesOptions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304f1450-39f5-4561-9eed-89851276be20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely import geometry\n",
    "from functools import reduce\n",
    "\n",
    "\n",
    "def to_polygon(boundingbox):\n",
    "    p1 = geometry.Point(boundingbox[\"left\"], boundingbox[\"top\"])\n",
    "    p2 = geometry.Point(boundingbox[\"left\"] + boundingbox[\"width\"], boundingbox[\"top\"])\n",
    "    p3 = geometry.Point(boundingbox[\"left\"], boundingbox[\"top\"] + boundingbox[\"height\"])\n",
    "    p4 = geometry.Point(\n",
    "        boundingbox[\"left\"] + boundingbox[\"width\"],\n",
    "        boundingbox[\"top\"] + boundingbox[\"height\"],\n",
    "    )\n",
    "    pointList = [p1, p2, p3, p4, p1]\n",
    "    return geometry.Polygon(pointList)\n",
    "\n",
    "\n",
    "def to_combined_boundingbox(boundingboxes):\n",
    "    combined = reduce(\n",
    "        lambda x, y: x.union(y), map(lambda b: to_polygon(b), boundingboxes)\n",
    "    )\n",
    "    x_min = combined.bounds[0]\n",
    "    y_min = combined.bounds[1]\n",
    "    x_max = combined.bounds[2]\n",
    "    y_max = combined.bounds[3]\n",
    "\n",
    "    boundingbox = {\n",
    "        \"left\": x_min,\n",
    "        \"top\": y_min,\n",
    "        \"width\": x_max - x_min,\n",
    "        \"height\": y_max - y_min,\n",
    "    }\n",
    "    return boundingbox\n",
    "\n",
    "\n",
    "questionOptions = estimatedBoundBoxesOptions[\"question\"].to_list()\n",
    "groupedOptions = estimatedBoundBoxesOptions.groupby([\"question\"]).apply(dict).to_dict()\n",
    "\n",
    "combined_boundingbox = dict(\n",
    "    map(\n",
    "        lambda q: (q, to_combined_boundingbox(groupedOptions[q][\"boundingBox\"])),\n",
    "        groupedOptions,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61580fb-0258-42f3-99e8-ec2e8aa43645",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "estimatedOptions = pd.DataFrame(\n",
    "    combined_boundingbox.items(), columns=[\"question\", \"boundingBox\"]\n",
    ").set_index(\"question\")\n",
    "questionToAnswer = df_answer[\"DocumentValue\"].iloc[0].to_dict()\n",
    "for key, value in questionToAnswer.items():\n",
    "    # do something with value\n",
    "    questionToAnswer[key] = value if type(value) is str else \"\"\n",
    "\n",
    "choices = [\"Yes\", \"No\", \"A\", \"B\", \"C\", \"D\", \"E\"]\n",
    "\n",
    "\n",
    "def get_answer(row):\n",
    "    queston = row.name\n",
    "    return set(\n",
    "        filter(\n",
    "            lambda x: x != \"\",\n",
    "            map(\n",
    "                lambda x: x\n",
    "                if queston + \"-\" + x in questionToAnswer\n",
    "                and questionToAnswer[queston + \"-\" + x] == \"X\"\n",
    "                else \"\",\n",
    "                choices,\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def get_choices(row):\n",
    "    queston = row.name\n",
    "    return list(\n",
    "        filter(\n",
    "            lambda x: x != \"\",\n",
    "            map(\n",
    "                lambda x: queston + \"-\" + x\n",
    "                if queston + \"-\" + x in questionToAnswer\n",
    "                else \"\",\n",
    "                choices,\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "estimatedOptions[\"answers\"] = estimatedOptions.apply(get_answer, axis=1)\n",
    "estimatedOptions[\"choices\"] = estimatedOptions.apply(get_choices, axis=1)\n",
    "estimatedOptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304490d6-b39d-40ba-b57a-5a98295ba426",
   "metadata": {},
   "outputs": [],
   "source": [
    "questionToAnswer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ec3be6-6751-48cb-ba56-5e4fec3b585a",
   "metadata": {},
   "source": [
    "# Generate Marking form"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad6c647-358d-4cee-b951-51e5a6250f44",
   "metadata": {},
   "source": [
    "Check for regeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59904666-8909-4feb-ab48-f5e4aeca4014",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "questionDir = os.path.join(os.getcwd(), \"output\", \"grading_form\", \"questions\")\n",
    "questionAndControl = {}\n",
    "for path, currentDirectory, files in os.walk(questionDir):\n",
    "    for file in files:\n",
    "        if file == \"control.json\":\n",
    "            question = path[len(questionDir) + 1 :]\n",
    "            f = open(os.path.join(path, file))\n",
    "            data = json.load(f)\n",
    "            if \"regenerate\" in data:\n",
    "                questionAndControl[question] = data\n",
    "            f.close()\n",
    "\n",
    "questionAndControl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b221413-12db-4341-90e5-d0e625aa24b4",
   "metadata": {},
   "source": [
    "Crop Answer image and ORC again.\n",
    "\n",
    "Easy Orc supports difference languages and you can check https://github.com/JaidedAI/EasyOCR\n",
    "\n",
    "This step takes time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1be20b-1f69-4c04-bcc6-c3a918abcf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageEnhance\n",
    "from IPython.display import display\n",
    "import easyocr\n",
    "import tempfile\n",
    "import boto3\n",
    "\n",
    "easyocrLanguages = [\"en\"]\n",
    "reader = easyocr.Reader(easyocrLanguages, gpu=True)\n",
    "rekognition = boto3.client('rekognition')\n",
    "minConfidence = 70\n",
    "\n",
    "df_script[\"DocumentValueOverrided\"] = df_script[\n",
    "    \"DocumentValue\"\n",
    "].copy()\n",
    "\n",
    "def get_rekognition_detect_text(imagePath):\n",
    "    with open(imagePath, 'rb') as image:\n",
    "        response = rekognition.detect_text(\n",
    "            Image={\n",
    "                'Bytes': image.read()\n",
    "            },\n",
    "            Filters={\n",
    "                'WordFilter': {\n",
    "                    'MinConfidence': minConfidence\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "        textDetections=response['TextDetections']        \n",
    "        texts = []\n",
    "        for text in textDetections:\n",
    "                if text['Type'] == 'LINE':\n",
    "                    texts.append(text['DetectedText'])\n",
    "        text = \" \".join(texts)        \n",
    "        return text\n",
    "    \n",
    "def ocr_question_image(image,overrdiedBoundingBox):\n",
    "    with Image.open(image) as im:\n",
    "        # The crop method from the Image module takes four coordinates as input.\n",
    "        # The right can also be represented as (left+width)\n",
    "        # and lower can be represented as (upper+height).\n",
    "        (left, top, right, lower) = (\n",
    "            overrdiedBoundingBox[\"left\"],\n",
    "            overrdiedBoundingBox[\"top\"],\n",
    "            overrdiedBoundingBox[\"left\"] + overrdiedBoundingBox[\"width\"],\n",
    "            overrdiedBoundingBox[\"top\"] + overrdiedBoundingBox[\"height\"],\n",
    "        )\n",
    "        # Here the image \"im\" is cropped and assigned to new variable im_crop\n",
    "        im_crop = im.crop((left, top, right, lower))        \n",
    "        imageEnhance = ImageEnhance.Sharpness(im_crop)  \n",
    "        # showing resultant image\n",
    "        im_crop = imageEnhance.enhance(3)\n",
    "        im_crop.save(\"temp.png\", format=\"png\")\n",
    "        # display(im_crop)\n",
    "        # this needs to run only once to load the model into memory\n",
    "    result = reader.readtext(\"temp.png\", detail=0)\n",
    "    easyocrText = \"\".join(result)\n",
    "    text = easyocrText\n",
    "    if easyocrText != \"\" and canCallAWS:\n",
    "        rekognitionText = get_rekognition_detect_text(\"temp.png\")\n",
    "        text = rekognitionText if rekognitionText !=\"\" else easyocrText\n",
    "    return text\n",
    "\n",
    "\n",
    "for question, control in questionAndControl.items():\n",
    "    if control[\"regenerate\"] == \"on\" and control[\"boundingBoxMode\"] != \"tractract\":\n",
    "        print(question, control)\n",
    "        if control[\"boundingBoxMode\"] == \"manual\":\n",
    "            overrdiedBoundingBox = {\n",
    "                \"left\": float(control[\"left\"]),\n",
    "                \"top\": float(control[\"top\"]),\n",
    "                \"height\": float(control[\"height\"]),\n",
    "                \"width\": float(control[\"width\"]),\n",
    "            }\n",
    "        else:\n",
    "            overrdiedBoundingBox = estimatedBoundBoxes.loc[[question]][\"boundingBox\"][0]       \n",
    "        images = df_script[\"DocumentAnswerImageLocal\"][question].to_list()\n",
    "        \n",
    "        texts = []\n",
    "        for image in images:\n",
    "            text = ocr_question_image(image, overrdiedBoundingBox)\n",
    "            # print(text)\n",
    "            # display(im_crop)\n",
    "            texts.append(text)\n",
    "        df_script[\"DocumentValueOverrided\"][question] = texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aeac197-6da9-45a9-b6f6-390447e3ec02",
   "metadata": {},
   "source": [
    "Remove NaN to empty string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef28c88e-568f-4c58-8d51-a25dc32d9e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_script[\"DocumentValueOverrided\"].fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b077910-45f3-46f9-a42a-5d51aa9537ff",
   "metadata": {},
   "source": [
    "Recalculate the answer similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2881d619-e430-4af2-af64-f3e08483fae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer(\"paraphrase-MiniLM-L6-v2\")\n",
    "\n",
    "df_script[\"DocumentAnswerSimilarityOverrided\"] = df_script[\n",
    "    \"DocumentAnswerSimilarity\"\n",
    "].copy()\n",
    "for question, control in questionAndControl.items():\n",
    "    if control[\"regenerate\"] != \"on\":\n",
    "        continue\n",
    "    print(\"Recalculate Similarity for \" + question)\n",
    "    overridedStandardAnswer = control[\"overridedStandardAnswer\"]\n",
    "    answers = df_script[\"DocumentValueOverrided\"][question].to_list()\n",
    "    answers.insert(0, overridedStandardAnswer)\n",
    "    # Compute embeddings\n",
    "    embeddings = model.encode(answers, convert_to_tensor=True)\n",
    "    # Compute cosine-similarities for each sentence with each other sentence\n",
    "    cosine_scores = util.pytorch_cos_sim(embeddings, embeddings)\n",
    "    # Find the pairs with the highest cosine similarity scores\n",
    "    pairs = []\n",
    "    for j in range(0, len(cosine_scores)):\n",
    "        pairs.append(float(cosine_scores[0][j]))   \n",
    "    #Empty answer similarity must be 0.\n",
    "    l = list(map(lambda x: (x[0],0) if x[0] == \"\" else x, zip(answers, pairs)))\n",
    "    similarties = list(list(zip(*l))[1])\n",
    "    similarties.pop(0)  \n",
    "    df_script[\"DocumentAnswerSimilarityOverrided\"][question] = similarties      \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375d5fe6-3fad-4994-84af-0ed67843a2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_table(dataframeName, question, newName):\n",
    "    tempTable = df_script[dataframeName].copy()\n",
    "    tempTable = tempTable[[question]]\n",
    "    tempTable.reset_index(inplace=True)\n",
    "    tempTable[\"RowNumber\"] = tempTable.index\n",
    "    tempTable.rename(columns={question: newName}, inplace=True)\n",
    "    return tempTable\n",
    "\n",
    "\n",
    "def get_question_df(question):\n",
    "    answerTable = extract_table(\"DocumentValueOverrided\", question, \"Answer\")\n",
    "    imageTable = extract_table(\"DocumentAnswerImageLocal\", question, \"Image\")\n",
    "    confidenceTable = extract_table(\"DocumentConfidence\", question, \"Confidence\")\n",
    "    similarityTable = extract_table(\n",
    "        \"DocumentAnswerSimilarityOverrided\", question, \"Similarity\"\n",
    "    )\n",
    "    geometryTable = extract_table(\"DocumentBoundingBoxImpute\", question, \"BoundingBox\")\n",
    "\n",
    "    dataTable = pd.merge(\n",
    "        answerTable, imageTable, on=\"RowNumber\", suffixes=(\"\", \"_remove\")\n",
    "    )\n",
    "    dataTable = pd.merge(\n",
    "        dataTable, confidenceTable, on=\"RowNumber\", suffixes=(\"\", \"_remove\")\n",
    "    )\n",
    "    dataTable = pd.merge(\n",
    "        dataTable, similarityTable, on=\"RowNumber\", suffixes=(\"\", \"_remove\")\n",
    "    )\n",
    "    dataTable = pd.merge(\n",
    "        dataTable, geometryTable, on=\"RowNumber\", suffixes=(\"\", \"_remove\")\n",
    "    )\n",
    "    dataTable.drop(\n",
    "        [i for i in dataTable.columns if \"_remove\" in i], axis=1, inplace=True\n",
    "    )\n",
    "\n",
    "    dataTable[\"Similarity\"] = dataTable[\"Similarity\"].fillna(0)\n",
    "    dataTable[\"Answer\"] = dataTable[\"Answer\"].fillna(\"\")\n",
    "    dataTable[\"Image\"] = dataTable[\"Image\"].str.replace(\"output/grading_form/\", \"\")\n",
    "\n",
    "    dataTable = dataTable.sort_values(by=[\"Similarity\"], ascending=False)\n",
    "    return dataTable\n",
    "\n",
    "\n",
    "def save_template_output(output, question, filename):\n",
    "    path = Path(\n",
    "        os.path.join(os.getcwd(), \"output\", \"grading_form\", \"questions\", question)\n",
    "    )\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "    path = Path(\n",
    "        os.path.join(\n",
    "            os.getcwd(), \"output\", \"grading_form\", \"questions\", question, filename\n",
    "        )\n",
    "    )\n",
    "    text_file = open(path, \"w\")\n",
    "    text_file.write(output)\n",
    "    text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b287cf5f-b1e4-4b4f-8428-643bf7780513",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from distutils.dir_util import copy_tree\n",
    "\n",
    "from_directory = os.path.join(os.getcwd(), \"templates\", \"javascript\")\n",
    "to_directory = os.path.join(os.getcwd(), \"output\", \"grading_form\", \"javascript\")\n",
    "copy_tree(from_directory, to_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e4783c-f1c7-45c1-83a1-c5ae5864f1ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from jinja2 import Template\n",
    "from jinja2 import Environment, FileSystemLoader\n",
    "\n",
    "file_loader = FileSystemLoader(\"templates\")\n",
    "env = Environment(loader=file_loader)\n",
    "template = env.get_template(\"index.html\")\n",
    "\n",
    "questions = df_script[\"DocumentBoundingBox\"].columns.to_list()\n",
    "\n",
    "\n",
    "textAnswer = []\n",
    "for question in questions:\n",
    "    optionsDf = estimatedBoundBoxesOptions[\n",
    "        estimatedBoundBoxesOptions.index.str.fullmatch(question)\n",
    "    ]\n",
    "    if optionsDf.empty:  # skip choice questions\n",
    "        textAnswer.append(question)\n",
    "optionAnswer = estimatedOptions.index.to_list()\n",
    "\n",
    "questionOfMarks = textAnswer + optionAnswer\n",
    "\n",
    "output = template.render(\n",
    "    studentsScriptFileName=studentsScriptFileName,\n",
    "    textAnswer=textAnswer,\n",
    "    optionAnswer=optionAnswer,\n",
    ")\n",
    "# open text file\n",
    "path = Path(os.path.join(os.getcwd(), \"output\", \"grading_form\", \"questions\"))\n",
    "path.mkdir(parents=True, exist_ok=True)\n",
    "path = Path(os.path.join(os.getcwd(), \"output\", \"grading_form\", \"index.html\"))\n",
    "text_file = open(path, \"w\")\n",
    "text_file.write(output)\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2d0e53-11b5-4f68-becd-6ffd244c7e2b",
   "metadata": {},
   "source": [
    "Generate grading form for text answer questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a4317b-6cb7-46f8-8569-2e0be3644f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = df_script[\"DocumentBoundingBox\"].columns.to_list()\n",
    "\n",
    "for question in questions:\n",
    "    optionsDf = estimatedBoundBoxesOptions[\n",
    "        estimatedBoundBoxesOptions.index.str.fullmatch(question)\n",
    "    ]\n",
    "    if not optionsDf.empty:  # skip choice questions\n",
    "        continue\n",
    "\n",
    "    dataTable = get_question_df(question)\n",
    "\n",
    "    standardAnswer = None\n",
    "    estimatedBoundingBox = None\n",
    "    if question in df_answer[\"DocumentValue\"].columns:\n",
    "        standardAnswer = questionToAnswer[question]\n",
    "    if question in df_script[\"DocumentBoundingBox\"].columns:\n",
    "        estimatedBoundingBox = estimatedBoundBoxes.loc[[question]][\"boundingBox\"][0]\n",
    "\n",
    "    if question == \"ID\" or question == \"Name\":\n",
    "        template = env.get_template(\"questions/index-answer.html\")\n",
    "    else:\n",
    "        template = env.get_template(\"questions/index.html\")\n",
    "    output = template.render(\n",
    "        studentsScriptFileName=studentsScriptFileName,\n",
    "        question=question,\n",
    "        standardAnswer=standardAnswer,\n",
    "        estimatedBoundingBox=estimatedBoundingBox,\n",
    "        dataTable=dataTable,\n",
    "    )\n",
    "    save_template_output(output, question, \"index.html\")\n",
    "\n",
    "    template = env.get_template(\"questions/question.js\")\n",
    "    output = template.render(\n",
    "        dataTable=dataTable,\n",
    "        estimatedBoundingBox=estimatedBoundingBox,\n",
    "    )\n",
    "    save_template_output(output, question, \"question.js\")\n",
    "\n",
    "    template = env.get_template(\"questions/style.css\")\n",
    "    output = template.render(\n",
    "        dataTable=dataTable,\n",
    "    )\n",
    "    save_template_output(output, question, \"style.css\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3073dbc9-25f8-4e60-90e2-87f1f2f3a0dc",
   "metadata": {},
   "source": [
    "Generate grading form for choices style questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615140e7-441c-48ab-89b4-3fb55960e1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in estimatedOptions.iterrows():\n",
    "    question = index\n",
    "    choices = row[\"choices\"]\n",
    "    estimatedBoundingBox = row[\"boundingBox\"]\n",
    "    answers = row[\"answers\"]\n",
    "    answersMask = list(\n",
    "        map(lambda c: 1 if c.replace(question + \"-\", \"\") in answers else 0, choices)\n",
    "    )\n",
    "    choicesDf = list(map(lambda o: (o, get_question_df(o)), choices))\n",
    "\n",
    "    rowNumber = choicesDf[0][1][\"RowNumber\"]\n",
    "    images = choicesDf[0][1][\"Image\"]\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        list(zip(rowNumber, images)), columns=[\"RowNumber\", \"Image\"], index=rowNumber\n",
    "    )\n",
    "\n",
    "    choiceDict = {}\n",
    "    for index, item in enumerate(choicesDf):\n",
    "        q, qdf = item\n",
    "        # Remove Image for option as it is a common column and convert to dict\n",
    "        choiceDict[q] = qdf.drop([\"Image\"], axis=1).to_dict(\"index\")\n",
    "\n",
    "    def get_confidence(index):\n",
    "        return min(map(lambda c: choiceDict[c][index][\"Confidence\"], choices))\n",
    "\n",
    "    def get_answer(index):\n",
    "        return \",\".join(\n",
    "            map(\n",
    "                lambda c: \"1\" if choiceDict[c][index][\"Answer\"] == \"X\" else \"0\", choices\n",
    "            )\n",
    "        )\n",
    "\n",
    "    df[\"choices\"] = pd.Series([choices for x in range(len(df.index))])\n",
    "    df[\"BoundingBox\"] = pd.Series([estimatedBoundingBox for x in range(len(df.index))])\n",
    "    df[\"answers\"] = pd.Series([answers for x in range(len(df.index))])\n",
    "    df[\"Answer\"] = pd.Series([get_answer(x) for x in range(len(df.index))])\n",
    "    df[\"Confidence\"] = pd.Series([get_confidence(x) for x in range(len(df.index))])\n",
    "\n",
    "    template = env.get_template(\"questions/index-choices.html\")\n",
    "    output = template.render(\n",
    "        studentsScriptFileName=studentsScriptFileName,\n",
    "        question=question,\n",
    "        standardAnswer=\" \".join(answers),\n",
    "        answersMask=answersMask,\n",
    "        estimatedBoundingBox=estimatedBoundingBox,\n",
    "        dataTable=df,\n",
    "        choiceDict=choiceDict,\n",
    "    )\n",
    "\n",
    "    save_template_output(output, question, \"index.html\")\n",
    "\n",
    "    template = env.get_template(\"questions/question.js\")\n",
    "    output = template.render(\n",
    "        dataTable=dataTable,\n",
    "        estimatedBoundingBox=estimatedBoundingBox,\n",
    "    )\n",
    "    save_template_output(output, question, \"question.js\")\n",
    "\n",
    "    template = env.get_template(\"questions/style.css\")\n",
    "    output = template.render(\n",
    "        dataTable=dataTable,\n",
    "    )\n",
    "    save_template_output(output, question, \"style.css\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf9d2e7-213b-437c-ba86-7b371c34ae11",
   "metadata": {},
   "source": [
    "# Start Python HTTPServer\n",
    "\n",
    "Copy your Juptyer notebook url i.e. \n",
    "\n",
    "https://xxxxx.studio.us-east-2.sagemaker.aws/studiolab/default/jupyter/lab/tree/sagemaker-studiolab-notebooks/marking_form_builder.ipynb\n",
    "\n",
    "Paste into a new broswer tab and change it to: \n",
    "\n",
    "https://xxxxx.studio.us-east-2.sagemaker.aws/studiolab/default/jupyter/proxy/8000/\n",
    "\n",
    "When you finished your grading tasks, you need to stop the webserver with Interrupt Kernel.\n",
    "\n",
    "The webserver log is in output/server.log.\n",
    "\n",
    "If you are in development and don't want the notebook being blocked by running webserver, you can open a terminal and run the below 2 commands.\n",
    "\n",
    "cd sagemaker-studiolab-notebooks/\n",
    "\n",
    "python server.py 8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b32f0ae-d5e3-4343-94c3-fd4cf19886bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python server.py 8000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193c302c-e38a-47b2-9b92-14bfd5d94c04",
   "metadata": {},
   "source": [
    "# Backup grading result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63f07bc-172a-41e7-b5eb-2e3041ae0ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.make_archive(\n",
    "    os.path.join(os.getcwd(), \"output\", \"grading_form\", \"questions\"),\n",
    "    \"zip\",\n",
    "    os.path.join(os.getcwd(), \"output\", \"grading_form\", \"questions\"),\n",
    ")\n",
    "shutil.make_archive(\n",
    "    os.path.join(os.getcwd(), \"output\"),\n",
    "    \"zip\",\n",
    "    os.path.join(os.getcwd(), \"output\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a1400d-8293-489c-9820-47cd5df03803",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\"Dowload question html files.\")\n",
    "display(os.path.join(os.getcwd(), \"output\", \"grading_form\", \"questions.zip\"))\n",
    "display(\"Dowload grading form files with everything.\")\n",
    "display(os.path.join(os.getcwd(), \"output.zip\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c766227a-b4ff-42a9-902e-07d1adac030b",
   "metadata": {},
   "source": [
    "# Generate Marksheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb134141-6b10-4216-927a-c63aff48c991",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "questionDir = os.path.join(os.getcwd(), \"output\", \"grading_form\", \"questions\")\n",
    "questionAndMarks = {}\n",
    "for path, currentDirectory, files in os.walk(questionDir):\n",
    "    for file in files:\n",
    "        if file == \"mark.json\":\n",
    "            question = path[len(questionDir) + 1 :]\n",
    "            f = open(os.path.join(path, file))\n",
    "            data = json.load(f)\n",
    "            marks = {}\n",
    "            for i in data:\n",
    "                marks[i[\"id\"]] = (\n",
    "                    i[\"overridedMark\"] if i[\"overridedMark\"] != \"\" else i[\"mark\"]\n",
    "                )\n",
    "            questionAndMarks[question] = marks\n",
    "            f.close()\n",
    "marksDf = pd.DataFrame(questionAndMarks)\n",
    "marksDf = marksDf[\n",
    "    [\"ID\", \"Name\"]\n",
    "    + [col for col in sorted(marksDf.columns) if col != \"ID\" and col != \"Name\"]\n",
    "]\n",
    "print(marksDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716ae771-46a9-4232-a0fb-4ca98686654e",
   "metadata": {},
   "source": [
    "The questionWithoutMarks should be an empty list and the list items are the question without mark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0f9a12-936e-481d-92bc-27c30974753b",
   "metadata": {},
   "outputs": [],
   "source": [
    "questionWithoutMarks = list(set(questionOfMarks) - set(marksDf.columns))\n",
    "questionWithoutMarks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c16379-e681-4cda-83ea-f0cb6c8f070e",
   "metadata": {},
   "source": [
    "Clean all cell outputs before commit to GitHub.\n",
    "\n",
    "```jupyter nbconvert --clear-output --inplace grading_form_builder.ipynb```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034e86e6-7076-4604-b33d-c50c276874a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic:Python",
   "language": "python",
   "name": "conda-env-basic-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
