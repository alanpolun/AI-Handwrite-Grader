{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Student Answer\n",
    "Create scoring web apps and validate the scoring result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install Linux tools and only required for the first run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt-get update\n",
    "!sudo apt-get install ffmpeg libsm6 libxext6 -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart kernel after installs so that your environment can access the new packages\n",
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = 'cyrus-testing-2023'\n",
    "!gcloud config set project {project_id}\n",
    "!gcloud auth application-default set-quota-project {project_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "import vertexai\n",
    "\n",
    "aiplatform.init(\n",
    "    # your Google Cloud Project ID or number\n",
    "    # environment default used is not set\n",
    "    project=project_id\n",
    ")\n",
    "\n",
    "vertexai.init(project=project_id, location=\"us-central1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vertexai.generative_models import GenerativeModel, Part\n",
    "import vertexai.preview.generative_models as generative_models\n",
    "\n",
    "def call_llm(prompt=None, max_output_tokens=1024, temperature=0, top_p=0.3):\n",
    "    if prompt is None:\n",
    "        raise ValueError(\"Prompt cannot be None\")\n",
    "    \n",
    "    model = GenerativeModel(\"gemini-1.0-pro-001\")\n",
    "    generation_config = {\n",
    "        \"max_output_tokens\": max_output_tokens,\n",
    "        \"temperature\": temperature,\n",
    "        \"top_p\": top_p,\n",
    "    }\n",
    "    safety_settings = {\n",
    "        generative_models.HarmCategory.HARM_CATEGORY_HATE_SPEECH: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "        generative_models.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "        generative_models.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "        generative_models.HarmCategory.HARM_CATEGORY_HARASSMENT: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "    }\n",
    "    \n",
    "    retry = 0\n",
    "    while retry < 3:\n",
    "        try:\n",
    "            responses = model.generate_content(\n",
    "                [prompt],\n",
    "                generation_config=generation_config,\n",
    "                safety_settings=safety_settings,\n",
    "                stream=True,\n",
    "            )\n",
    "            text = \"\"\n",
    "            for response in responses:\n",
    "                text += response.text\n",
    "            return text\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            retry += 1        \n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def call_llm_visual(prompt:str, filePath:str):    \n",
    "    model = GenerativeModel(\"gemini-1.0-pro-vision-001\")    \n",
    "    with open(filePath, \"rb\") as f:\n",
    "        data = f.read()\n",
    "    image1 = Part.from_data(mime_type=\"image/png\", data=data)\n",
    "    generation_config = {\n",
    "        \"max_output_tokens\": 2048,\n",
    "        \"temperature\": 0,\n",
    "        \"top_p\": 0.5,\n",
    "    }\n",
    "    safety_settings = {\n",
    "        generative_models.HarmCategory.HARM_CATEGORY_HATE_SPEECH: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "        generative_models.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "        generative_models.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "        generative_models.HarmCategory.HARM_CATEGORY_HARASSMENT: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "    }\n",
    "    responses = model.generate_content(\n",
    "        [image1, prompt],\n",
    "        generation_config=generation_config,\n",
    "        safety_settings=safety_settings,\n",
    "        stream=True,\n",
    "    )\n",
    "    text = \"\"\n",
    "    for response in responses:        \n",
    "        text += response.text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = \"\"\"\n",
    "Extract question and answer from the image.\n",
    "Each Cell contains a question and answer.\n",
    "Question is the printed text in left upper coner of the cell \n",
    "Question is informat 1), 2), 3)...\n",
    "Asnwer is in handwrting text after the question's ')'.\n",
    "\n",
    "Special handling 3 optional fields in the top of image, and they are not in cell.\n",
    "1. Name: <- Question is \"Name\", answer is hand writing text.\n",
    "2. Student ID: <- Question is \"StudentID\", answer is hand writing text.\n",
    "3. Class: <- Question is \"Class\", answer is hand writing text.\n",
    "\n",
    "Return JSON arrray object with question and answer.\n",
    "{\n",
    "    'question': '1)',\n",
    "    'answer': 'HAND WRITING TEXT'\n",
    "}\n",
    "Output:\n",
    "\"\"\"\n",
    "a = call_llm_visual(p, \"/workspaces/ai-scoring-handwriting-assignment/marking_form/TestScript/images/0.jpg\")\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_file = \"../data/TestScript.pdf\"\n",
    "standard_answer = pdf_file.replace(\".pdf\", \".xlsx\")\n",
    "question_with_answer_docx= pdf_file.replace(\"TestScript.pdf\", \"QuestionAndAsnwer.docx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "file_name = os.path.basename(pdf_file)\n",
    "file_name = os.path.splitext(file_name)[0]\n",
    "base_path = \"../marking_form/\" + file_name\n",
    "base_path_images = base_path + \"/images/\"\n",
    "base_path_annotations = base_path+\"/annotations/\"\n",
    "base_path_questions = base_path+\"/questions\"\n",
    "base_path_javascript = base_path+\"/javascript\"\n",
    "extracted_questions_excel = base_path + \"/extracted_questions.xlsx\"\n",
    "extracted_questions_review_excel = base_path + \"/extracted_questions_review.xlsx\"\n",
    "\n",
    "# create directory tree for base_path_images\n",
    "os.makedirs(base_path_images, exist_ok=True)\n",
    "os.makedirs(base_path_annotations, exist_ok=True)\n",
    "os.makedirs(base_path_questions, exist_ok=True)\n",
    "os.makedirs(base_path_javascript, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "annotations_path = base_path_annotations + \"annotations.json\"\n",
    "with open(annotations_path, \"r\") as f: \n",
    "    annotations = json.load(f)          \n",
    "\n",
    "#flatten annotations to list \n",
    "annotations_list = []\n",
    "for page in annotations:\n",
    "    for annotation in annotations[page]:\n",
    "        annotation[\"page\"] = int(page)\n",
    "        # x to left, y to top\n",
    "        annotation[\"left\"] = annotation[\"x\"]\n",
    "        annotation[\"top\"] = annotation[\"y\"]\n",
    "        annotation.pop(\"x\")\n",
    "        annotation.pop(\"y\")\n",
    "        annotations_list.append(annotation) \n",
    "annotations_list\n",
    "\n",
    "# convert annotations_list to dict with key with label\n",
    "annotations_dict = {}\n",
    "for annotation in annotations_list:\n",
    "    annotations_dict[annotation[\"label\"]] = annotation\n",
    "# annotations_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract list of label from annotations as questions\n",
    "questions = []\n",
    "for annotation in annotations_list:\n",
    "    if annotation[\"label\"] not in questions:\n",
    "        questions.append(annotation[\"label\"])\n",
    "# remove 'NAME', 'ID', 'CLASS' if exists in questions\n",
    "if 'NAME' in questions:\n",
    "    questions.remove('NAME')\n",
    "if 'ID' in questions:\n",
    "    questions.remove('ID')\n",
    "if 'CLASS' in questions:\n",
    "    questions.remove('CLASS')    \n",
    "\n",
    "# sort questions \n",
    "questions.sort()\n",
    "# question_with_answer = questions.copy()\n",
    "questions = ['NAME', 'ID', 'CLASS'] + questions\n",
    "questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Question, Answer and mark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mammoth\n",
    "\n",
    "def convert_image(image):  \n",
    "    return {\n",
    "        \"src\": \"\"\n",
    "    }\n",
    "\n",
    "with open(\"/workspaces/ai-scoring-handwriting-assignment/data/Sample.docx\", \"rb\") as docx_file:\n",
    "    result = mammoth.convert_to_html(docx_file, convert_image=mammoth.images.img_element(convert_image))\n",
    "    html  = result.value # The generated HTML\n",
    "    messages = result.messages # Any messages, such as warnings during conversion\n",
    "html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_question_answer(raw_text,questions):\n",
    "    prompt = f\"\"\"\n",
    "    Example 1 - single-level question HTML structure:\n",
    "    ++++++++++++++++++++++++++\n",
    "    Session2.2: The following questions Governance and Compliance.    \n",
    "    18)\tHow can an Azure region with 1 available zone to ensure high availability? [1 mark]\n",
    "    regions are paired\n",
    "    ++++++++++++++++++++++++++\n",
    "\n",
    "    Example 1 - JSON structure output:\n",
    "    ++++++++++++++++++++++++++\n",
    "    [\n",
    "        {{\n",
    "            \"questionId\":\"18\", \n",
    "            \"parent\": null,\n",
    "            \"question\":\"How can an Azure region with 1 available zone to ensure high availability?\",\n",
    "            \"answer\":\"regions are paired\",\n",
    "            \"mark\": 1,\n",
    "        }}\n",
    "    ]\n",
    "    ++++++++++++++++++++++++++\n",
    "\n",
    "    \n",
    "    Example 2 - mult-level question HTML structure:\n",
    "    ++++++++++++++++++++++++++  \n",
    "    Session2.2: The following questions Governance and Compliance.    \n",
    "        19)\tWith pandas data frame (df), write down the program code for the following cases:  \n",
    "            a.\tTo preview first 5 rows data. [1 mark]\n",
    "            df.head(5)\n",
    "    ++++++++++++++++++++++++++ \n",
    "    Example 2 - mult-level JSON structure output:\n",
    "    ++++++++++++++++++++++++++\n",
    "    [     \n",
    "        {{\n",
    "            \"questionId\":\"19a\",\n",
    "            \"parent\":\"With pandas data frame (df), write down the program code for the following cases: \"\n",
    "            \"question\":\"To preview first 5 rows data.\",\n",
    "            \"answer\":\"df.head(5)\",\n",
    "            \"mark\": 1,\n",
    "        }}\n",
    "    ]\n",
    "    ++++++++++++++++++++++++++\n",
    "\n",
    "    1. questionId can be mult-level e.g. 19a, 19b, ...etc.\n",
    "    2. mult-level question includes parent question. \n",
    "    3. The questionId should be in the following list.\n",
    "        {questions}\n",
    "\n",
    "    The content of the HTML file is as follows:\n",
    "    <INPUT>\n",
    "    \n",
    "    {raw_text}\n",
    "\n",
    "    </INPUT>    \n",
    "\n",
    "    IMPORTANT: \n",
    "    1. The output must be a JSON array of object contains only 'questionId', 'parent', 'question', 'answer', and 'mark'!\n",
    "    2. no duplicate attribute.\n",
    "    3. Escaped unescaped character in value string!\n",
    "    4. Don't output ``` at the end of the JSON array.\n",
    "    OUTPUT:\n",
    "    \"\"\"\n",
    "    return call_llm(prompt,max_output_tokens=8192)\n",
    "questions_string = ', '.join(questions)\n",
    "raw_result = get_question_answer(html,questions_string)\n",
    "print(raw_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It always return ``` after the end of JSON array!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_occurrence = raw_result.rfind(']')\n",
    "if last_occurrence != -1:\n",
    "    result = raw_result[:last_occurrence + 1]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.loads(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import Optional\n",
    "\n",
    "class Question(BaseModel):\n",
    "    questionId: str\n",
    "    parent: Optional[str]\n",
    "    question: str\n",
    "    answer: str\n",
    "    mark: int\n",
    "\n",
    "errors_questions = []\n",
    "extracted_questions = []\n",
    "for item in json.loads(result):\n",
    "    try:\n",
    "        q = Question(**item)\n",
    "        extracted_questions.append(q)\n",
    "    except Exception as e: \n",
    "        errors_questions.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert extracted_questions list to DataFrame\n",
    "extracted_questions_df = pd.DataFrame([vars(s) for s in extracted_questions])\n",
    "errors_questions_df = pd.DataFrame(errors_questions)\n",
    "\n",
    "# Create a Pandas Excel writer using the file name\n",
    "with pd.ExcelWriter(extracted_questions_excel) as writer:\n",
    "    # Write the errors_questions_df DataFrame to a sheet named 'Errors'\n",
    "    errors_questions_df.to_excel(writer, sheet_name='Errors', index=False)    \n",
    "    # Write the extracted_questions_df DataFrame to a sheet named 'Extracted Questions'\n",
    "    extracted_questions_df.to_excel(writer, sheet_name='Extracted Questions', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You need to Review the file fix the incorrect data manually.\n",
    "Rename extracted_questions_excel.xlsx to extracted_questions_review_excel.xlsx.\n",
    "Load back the excel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Load the DataFrame from the \"Extracted Questions\" sheet\n",
    "standard_answer_df = pd.read_excel(extracted_questions_review_excel, sheet_name='Extracted Questions')\n",
    "\n",
    "# Print the loaded DataFrame\n",
    "standard_answer_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Student Name List."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load standard_answer to dataframe\n",
    "import pandas as pd\n",
    "name_list_df = pd.read_excel(standard_answer, sheet_name=\"NameList\")\n",
    "name_list_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check any unmatch between Annotation and answer from Gemini Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from termcolor import colored\n",
    "\n",
    "for question in questions:\n",
    "    if question not in standard_answer_df[\"questionId\"].values:\n",
    "        print(colored(\"Question {} is not in standard_answer!\".format(question), 'red'))\n",
    "\n",
    "for question in standard_answer_df[\"questionId\"].values:\n",
    "    if question not in questions:\n",
    "        print(colored(\"Question {} is not in annotations!\".format(question), 'red'))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_answer = standard_answer_df.set_index(\"Question\").to_dict()[\"Answer\"]\n",
    "standard_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_mark = standard_answer_df.set_index(\"Question\").to_dict()[\"Mark\"]\n",
    "standard_mark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for the regeneration of question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "questionAndControl = {}\n",
    "for path, currentDirectory, files in os.walk(base_path_questions):\n",
    "    for file in files:\n",
    "        if file == \"control.json\":\n",
    "            question = path[len(base_path_questions) + 1 :]\n",
    "            f = open(os.path.join(path, file))\n",
    "            data = json.load(f)\n",
    "            if \"regenerate\" in data:\n",
    "                questionAndControl[question] = data\n",
    "            f.close()\n",
    "\n",
    "questionAndControl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distutils.dir_util import copy_tree\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "from_directory = os.path.join(os.getcwd(), \"..\",\"templates\", \"javascript\")\n",
    "copy_tree(from_directory, base_path_javascript)\n",
    "ico = os.path.join(os.getcwd(), \"..\",\"templates\", \"favicon.ico\")\n",
    "# copy ico file  to base_path\n",
    "shutil.copyfile(ico, base_path+\"/favicon.ico\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from jinja2 import Environment, FileSystemLoader\n",
    "\n",
    "file_loader = FileSystemLoader(\"../templates\")\n",
    "env = Environment(loader=file_loader)\n",
    "template = env.get_template(\"index.html\")\n",
    "\n",
    "output = template.render(\n",
    "    studentsScriptFileName=file_name,\n",
    "    textAnswer=questions,\n",
    "    optionAnswer=[],\n",
    ")\n",
    "# open text file\n",
    "path = Path(os.path.join(base_path, \"index.html\"))\n",
    "text_file = open(path, \"w\")\n",
    "text_file.write(output)\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vertexai.generative_models import GenerativeModel, Part\n",
    "import vertexai.preview.generative_models as generative_models\n",
    "\n",
    "def ocr(prompt:str, filePath:str):    \n",
    "    model = GenerativeModel(\"gemini-1.0-pro-vision-001\")    \n",
    "    with open(filePath, \"rb\") as f:\n",
    "        data = f.read()\n",
    "    image1 = Part.from_data(mime_type=\"image/png\", data=data)\n",
    "    generation_config = {\n",
    "        \"max_output_tokens\": 2048,\n",
    "        \"temperature\": 0,\n",
    "        \"top_p\": 0.5,\n",
    "    }\n",
    "    safety_settings = {\n",
    "        generative_models.HarmCategory.HARM_CATEGORY_HATE_SPEECH: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "        generative_models.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "        generative_models.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "        generative_models.HarmCategory.HARM_CATEGORY_HARASSMENT: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "    }\n",
    "    responses = model.generate_content(\n",
    "        [image1, prompt],\n",
    "        generation_config=generation_config,\n",
    "        safety_settings=safety_settings,\n",
    "        stream=True,\n",
    "    )\n",
    "    text = \"\"\n",
    "    for response in responses:        \n",
    "        text += response.text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "from PIL import Image, ImageEnhance\n",
    "\n",
    "def ocr_image_from_file(question, image_path, left, top, width, height):\n",
    "    if question == \"NAME\" :\n",
    "        return \"\"\n",
    "    \n",
    "    imageFile = tempfile.NamedTemporaryFile(suffix=\".png\").name\n",
    "    with Image.open(image_path) as im:\n",
    "        # The crop method from the Image module takes four coordinates as input.\n",
    "        # The right can also be represented as (left+width)\n",
    "        # and lower can be represented as (upper+height).\n",
    "        (left, top, right, lower) = (\n",
    "            left,\n",
    "            top,\n",
    "            left + width,\n",
    "            top + height,\n",
    "        )\n",
    "        # Here the image \"im\" is cropped and assigned to new variable im_crop\n",
    "        im_crop = im.crop((left, top, right, lower))\n",
    "        imageEnhance = ImageEnhance.Sharpness(im_crop)\n",
    "        # showing resultant image\n",
    "        im_crop = imageEnhance.enhance(3)\n",
    "        im_crop.save(imageFile, format=\"png\")\n",
    "        \n",
    "    if question == \"ID\" :\n",
    "        text_message = \"\"\"\n",
    "            Extract text in this image.\n",
    "            It is a Student ID in 9 digit number.\n",
    "            Answer just extracted Student ID and don't answer anything else.\n",
    "            If you cannot extract Student ID, please return 'No text found!!!'.\n",
    "            \"\"\"\n",
    "    else:    \n",
    "        text_message =\"\"\"\n",
    "            Extract text in this image in English and number.\n",
    "            Answer just extracted text and don't answer anything else.\n",
    "            If you cannot extract text, please return 'No text found!!!'.\"\"\"       \n",
    "\n",
    "    try:\n",
    "        ocr_text = ocr(text_message, imageFile)  \n",
    "        print(question, image_path ,ocr_text) \n",
    "        if ocr_text == \"No text found!!!\":\n",
    "            return \"\"\n",
    "        return ocr_text\n",
    "    except Exception as e:\n",
    "        print(question, image_path, e)    \n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change Student Answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vertexai.generative_models import GenerativeModel, Part, FinishReason\n",
    "import vertexai.preview.generative_models as generative_models\n",
    "\n",
    "def similarity_score(standard_answer, submitted_answer):\n",
    "    model = GenerativeModel(\"gemini-1.0-pro-001\")\n",
    "    generation_config = {\n",
    "        \"max_output_tokens\": 1024,\n",
    "        \"temperature\": 0,\n",
    "        \"top_p\": 0.3,\n",
    "    }\n",
    "    safety_settings = {\n",
    "        generative_models.HarmCategory.HARM_CATEGORY_HATE_SPEECH: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "        generative_models.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "        generative_models.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "        generative_models.HarmCategory.HARM_CATEGORY_HARASSMENT: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "    }\n",
    "    prompt = f'''\n",
    "You are a grader to give a single digit floating point score from 0 to 1.\n",
    "Give the score according to the similarity of the meaning between the standard answer and submitted answer.\n",
    "Only reply a floating point number.\n",
    "Standard answer:\n",
    "\n",
    "{standard_answer}\n",
    "\n",
    "Submitted Answer:\n",
    "\n",
    "{submitted_answer}\n",
    "\n",
    "Score: \n",
    "'''\n",
    "    retry = 0\n",
    "    while retry < 3:\n",
    "        responses = model.generate_content(\n",
    "            [prompt],\n",
    "            generation_config=generation_config,\n",
    "            safety_settings=safety_settings,\n",
    "            stream=True,\n",
    "        )\n",
    "        text = \"\"\n",
    "        for response in responses:\n",
    "            text += response.text\n",
    "        try:\n",
    "            score = float(text)\n",
    "            return score\n",
    "            break  # If the conversion is successful, break the loop\n",
    "        except ValueError:\n",
    "            print(\"Retry\")\n",
    "            retry += 1\n",
    "            continue\n",
    "    return 0\n",
    "        \n",
    "\n",
    "def calculate_similarity(answers, question):\n",
    "    # Add the standard answer to the head of list.\n",
    "    if question not in standard_answer:\n",
    "        ## return list of 0 in len of answers\n",
    "        return [0] * len(answers)\n",
    "    answer = standard_answer[question]\n",
    "    scores = []\n",
    "    for submitted_answer in answers:\n",
    "        submitted_answer = str(submitted_answer)\n",
    "        if submitted_answer.strip() == \"\":\n",
    "            scores.append(0)\n",
    "            continue\n",
    "        score = similarity_score(answer, submitted_answer)\n",
    "        scores.append(score)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def get_the_list_of_files(path):\n",
    "    \"\"\"\n",
    "    Get the list of files in the directory\n",
    "    \"\"\"\n",
    "    files = []\n",
    "    for dirpath, dirnames, filenames in os.walk(path):\n",
    "        files.extend(filenames)\n",
    "        break\n",
    "    return sorted(files)\n",
    "\n",
    "\n",
    "images = get_the_list_of_files(base_path_images)\n",
    "\n",
    "# get max page from annotations_list\n",
    "max_page = 0\n",
    "for annotation in annotations_list:\n",
    "    if annotation[\"page\"] > max_page:\n",
    "        max_page = annotation[\"page\"]\n",
    "max_page = max_page + (1 if max_page % 2 == 1 else max_page + 2) # Scanner will have a blank page!\n",
    "\n",
    "# filter images by file name divided by page\n",
    "images_by_page = []\n",
    "for page in range(max_page):\n",
    "    images_by_page.append([])\n",
    "    for image in images:\n",
    "        p = int(image.split(\".\")[0])\n",
    "        if p % max_page == page:\n",
    "            images_by_page[page].append(image)\n",
    "\n",
    "\n",
    "def get_df(question):\n",
    "    row = annotations_dict[question].copy()\n",
    "\n",
    "    row[\"Confidence\"] = 0.1\n",
    "    row[\"Similarity\"] = 0\n",
    "    row[\"Image\"] = images_by_page[row[\"page\"]]\n",
    "    # append base_path_images to each image\n",
    "    row[\"Image\"] = [\"images/\" + image for image in row[\"Image\"]]\n",
    "\n",
    "    # expend row to dataframe for each image in row[\"Image\"]\n",
    "    data = pd.DataFrame(row)\n",
    "    data = data.explode(\"Image\")\n",
    "    data = data.reset_index(drop=True)\n",
    "\n",
    "    data[\"Answer\"] = data.apply(\n",
    "        lambda row: ocr_image_from_file(question,\n",
    "            base_path + \"/\" + row[\"Image\"],\n",
    "            row[\"left\"],\n",
    "            row[\"top\"],\n",
    "            row[\"width\"],\n",
    "            row[\"height\"],\n",
    "        ),\n",
    "        axis=1,\n",
    "    )\n",
    "    # add column RowNumber\n",
    "    data[\"RowNumber\"] = data.index + 1\n",
    "    data[\"maskPage\"] = data[\"page\"]\n",
    "\n",
    "    similarties = calculate_similarity(data[\"Answer\"].tolist(), question)\n",
    "\n",
    "    data[\"Similarity\"] = similarties\n",
    "\n",
    "    data[\"page\"] = data[\"Image\"].apply(\n",
    "        lambda x: x.replace(\"images/\", \"\").replace(\".jpg\", \"\")\n",
    "    )\n",
    "    data[\"Mark\"] = data[\"Answer\"].apply(lambda x: \"0\" if len(x.strip()) == 0 else \"\")\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def save_template_output(output, question, filename):\n",
    "    path = Path(base_path_questions, question)\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "    path = Path(os.path.join(path, filename))\n",
    "    text_file = open(path, \"w\")\n",
    "    text_file.write(output)\n",
    "    text_file.close()\n",
    "\n",
    "\n",
    "# question = \"NAME\"\n",
    "# get_df(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate individual question page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "\n",
    "max_count = len(questions)\n",
    "f = IntProgress(min=0, max=max_count) # instantiate the bar\n",
    "display(f) # display the bar\n",
    "\n",
    "# for question in questions:\n",
    "#     dataTable = get_df(question)\n",
    "#     os.makedirs(base_path_questions + \"/\" + question, exist_ok=True)\n",
    "#     dataTable.to_csv(base_path_questions + \"/\" + question + \"/data.csv\", index=False)\n",
    "\n",
    "#     if question == \"ID\" or question == \"NAME\" or question == \"CLASS\":\n",
    "#         template = env.get_template(\"questions/index-answer.html\")\n",
    "#     else:\n",
    "#         template = env.get_template(\"questions/index.html\")\n",
    "#     output = template.render(\n",
    "#         studentsScriptFileName=file_name,\n",
    "#         question=question,\n",
    "#         standardAnswer=standard_answer[question] if question in standard_answer else \"\",\n",
    "#         standardMark=standard_mark[question] if question in standard_mark else \"\",\n",
    "#         estimatedBoundingBox=annotations_dict[question],\n",
    "#         dataTable=dataTable,\n",
    "#     )\n",
    "#     save_template_output(output, question, \"index.html\")\n",
    "\n",
    "#     template = env.get_template(\"questions/question.js\")\n",
    "#     output = template.render(\n",
    "#         dataTable=dataTable,\n",
    "#         estimatedBoundingBox=annotations_dict[question],\n",
    "#     )\n",
    "#     save_template_output(output, question, \"question.js\")\n",
    "\n",
    "#     template = env.get_template(\"questions/style.css\")\n",
    "#     output = template.render(\n",
    "#         dataTable=dataTable,\n",
    "#     )\n",
    "#     save_template_output(output, question, \"style.css\")\n",
    "#     f.value += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "\n",
    "max_count = len(questions)\n",
    "f = IntProgress(min=0, max=max_count) # instantiate the bar\n",
    "display(f) # display the bar\n",
    "\n",
    "for question in questions:\n",
    "    # dataTable = get_df(question)\n",
    "    # os.makedirs(base_path_questions + \"/\" + question, exist_ok=True)\n",
    "    # dataTable.to_csv(base_path_questions + \"/\" + question + \"/data.csv\", index=False)\n",
    "\n",
    "    data_path = base_path_questions + \"/\" + question + \"/data.csv\"\n",
    "    dataTable = pd.read_csv(data_path)\n",
    "    dataTable = dataTable.replace(\".*No text found!!!.*\", \"\", regex=True)\n",
    "    similarties = calculate_similarity(dataTable[\"Answer\"].tolist(), question)\n",
    "    dataTable[\"Similarity\"] = similarties\n",
    "    dataTable.to_csv(base_path_questions + \"/\" + question + \"/data.csv\", index=False)\n",
    "\n",
    "    if question == \"ID\" or question == \"NAME\" or question == \"CLASS\":\n",
    "        template = env.get_template(\"questions/index-answer.html\")\n",
    "    else:\n",
    "        template = env.get_template(\"questions/index.html\")\n",
    "    output = template.render(\n",
    "        studentsScriptFileName=file_name,\n",
    "        question=question,\n",
    "        standardAnswer=standard_answer[question] if question in standard_answer else \"\",\n",
    "        standardMark=standard_mark[question] if question in standard_mark else \"\",\n",
    "        estimatedBoundingBox=annotations_dict[question],\n",
    "        dataTable=dataTable,\n",
    "    )\n",
    "    save_template_output(output, question, \"index.html\")\n",
    "\n",
    "    template = env.get_template(\"questions/question.js\")\n",
    "    output = template.render(\n",
    "        dataTable=dataTable,\n",
    "        estimatedBoundingBox=annotations_dict[question],\n",
    "    )\n",
    "    save_template_output(output, question, \"question.js\")\n",
    "\n",
    "    template = env.get_template(\"questions/style.css\")\n",
    "    output = template.render(\n",
    "        dataTable=dataTable,\n",
    "    )\n",
    "    save_template_output(output, question, \"style.css\")\n",
    "    f.value += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate Student ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load csv file to dataframe\n",
    "import pandas as pd\n",
    "\n",
    "id_from_oscr = pd.read_csv(base_path_questions + \"/\" + \"ID\" + \"/data.csv\")[\"Answer\"].tolist()\n",
    "id_from_oscr = [str(int(float(x))) if pd.notna(x) else x for x in id_from_oscr]\n",
    "\n",
    "id_from_namelist = name_list_df[\"StudentID\"].to_list()\n",
    "\n",
    "# check duplicate id\n",
    "duplicate_id = []\n",
    "for id in id_from_oscr:\n",
    "    if id_from_oscr.count(id) > 1:\n",
    "        duplicate_id.append(id)\n",
    "duplicate_id = list(set(duplicate_id))\n",
    "if len(duplicate_id) > 0:\n",
    "    print(colored(\"Duplicate ID: {}\".format(duplicate_id), \"red\"))\n",
    "\n",
    "id_from_oscr = [str(id) for id in id_from_oscr]\n",
    "id_from_namelist = [str(id) for id in id_from_namelist]\n",
    "\n",
    "# compare oscr_id and validate_id\n",
    "ocr_missing_id = []\n",
    "name_list_missing_id = []\n",
    "for id in id_from_oscr:    \n",
    "    if id not in id_from_namelist:       \n",
    "        name_list_missing_id.append(id)\n",
    "\n",
    "for id in id_from_namelist:\n",
    "    if id not in id_from_oscr:   \n",
    "        ocr_missing_id.append(id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OCR scan error case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from termcolor import colored\n",
    "if len(ocr_missing_id) > 0:\n",
    "    print(colored(\"Some IDs OCR is not in NameList and you need to fix it manually!\", \"red\"))\n",
    "    for id in name_list_missing_id:\n",
    "        print(colored(id, \"red\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Potential Absent Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from termcolor import colored\n",
    "\n",
    "if len(ocr_missing_id) > 0:\n",
    "    print(colored(\"Number of absentee {}.\".format(len(ocr_missing_id)), \"red\"))\n",
    "    print(colored(\"ID in Name List does not find from OCR!\", \"red\"))\n",
    "    for id in ocr_missing_id:\n",
    "        print(colored(id, \"red\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Python HTTPServer\n",
    "\n",
    "The webserver log is in output/server.log.\n",
    "\n",
    "If you are in development and don't want the notebook being blocked by running webserver, you can open a terminal and run the below command.\n",
    "\n",
    "file_name=XXXX python server.py 8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"file_name={} python server.py\".format(file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also uncomment the following line to run the web server but if it crashes, you need to restart the kernel.\n",
    "# !cd .. && file_name=TestScript python server.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post Processing after scoring\n",
    "1. Check all question has scores.\n",
    "2. Check ID again.\n",
    "3. Remove version history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check each sub folder of base_path_questions contains file name mark.json, ignore the root folder\n",
    "import os\n",
    "import json\n",
    "\n",
    "unfinsihed_scoring = []\n",
    "for path, currentDirectory, files in os.walk(base_path_questions):\n",
    "    if path != base_path_questions:\n",
    "        # extract question name from path\n",
    "        question = path[len(base_path_questions) + 1 :]\n",
    "        if \"mark.json\" not in files:\n",
    "            unfinsihed_scoring.append(question)\n",
    "        else:\n",
    "            # read mark.json as json\n",
    "            with open(os.path.join(path, \"mark.json\"), \"r\") as f:\n",
    "                marks = json.load(f)            \n",
    "            # check each mark in marks that attribute \"mark\" or \"overridedMark\" is not empty\n",
    "            for mark in marks:\n",
    "                if mark['mark'] == \"\" and  mark['overridedMark'] == \"\":\n",
    "                    # extract question name from path                   \n",
    "                    unfinsihed_scoring.append(question)\n",
    "                    break             \n",
    "\n",
    "if len(unfinsihed_scoring) > 0:            \n",
    "    print(colored(\"{} have some question not yet mark!\".format(unfinsihed_scoring), \"red\"))          \n",
    "else:\n",
    "    print(\"All questions have been marked!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from termcolor import colored\n",
    "\n",
    "with open(os.path.join(base_path_questions,\"ID\", \"mark.json\"), \"r\") as f:\n",
    "    marks = json.load(f)\n",
    "\n",
    "id_from_mark = list(map(lambda x: x[\"overridedMark\"] if x[\"overridedMark\"] != \"\" else x[\"mark\"], marks))\n",
    "id_from_namelist = name_list_df[\"StudentID\"].to_list()\n",
    "\n",
    "# convert id_from_mark to string\n",
    "id_from_mark = [str(id) for id in id_from_mark]\n",
    "id_from_namelist = [str(id) for id in id_from_namelist]\n",
    "\n",
    "mark_missing_id = []\n",
    "for id in id_from_namelist:\n",
    "    if id not in id_from_mark:   \n",
    "        mark_missing_id.append(id)\n",
    "print(colored(\"In class but not marked - {}!\".format(mark_missing_id), \"red\"))    \n",
    "\n",
    "marked_but_not_in_namelist = []\n",
    "for id in id_from_mark:\n",
    "    if id not in id_from_namelist:   \n",
    "        marked_but_not_in_namelist.append(id)\n",
    "\n",
    "print(colored(\"Marked ID but not in class - {}!\".format(marked_but_not_in_namelist), \"red\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove version history\n",
    "Before you backup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## remove fill start with control- or mark- and end with .json in base_path_questions recursively.\n",
    "import os\n",
    "for path, currentDirectory, files in os.walk(base_path_questions):\n",
    "    for file in files:\n",
    "        if file.startswith(\"control-\") or file.startswith(\"mark-\"):\n",
    "            os.remove(os.path.join(path, file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reset everything (Danger)\n",
    "Remove mark.js and control.js"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# for path, currentDirectory, files in os.walk(base_path_questions):\n",
    "    \n",
    "#     for file in files:       \n",
    "#         if file == \"control.json\" or file == \"mark.json\":\n",
    "#             os.remove(os.path.join(path, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
